#!/usr/bin/env python3

import sys
import numpy as np

import argparse
import torch
import cv2
import pyzed.sl as sl
from ultralytics import YOLO

from threading import Lock, Thread
from time import sleep

import ogl_viewer.viewer as gl
import cv_viewer.tracking_viewer as cv_viewer
### ADDED ###
import datetime
import open3d as o3d


# =========================================================================
# === Class 1 (Pallet Top) Processing Logic (using Open3D) ===
# =========================================================================

def process_class1_pointcloud(pcd, max_angular_distance=5):
    """
    Class 1 (íŒ”ë ˆíŠ¸ ìœ—ë©´) í¬ì¸íŠ¸ í´ë¼ìš°ë“œì— ëŒ€í•´ í´ëŸ¬ìŠ¤í„°ë§, PCA ê¸°ë°˜ RANSACì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    print("\nâš™ï¸ Starting Class 1 PCA-informed RANSAC...")

    # 1. í´ëŸ¬ìŠ¤í„°ë§ ë° ê°€ì¥ í° ê·¸ë£¹ ì„ íƒ (DBSCAN ì‚¬ìš©)
    with o3d.utility.VerbosityContextManager(o3d.utility.VerbosityLevel.Error) as vcm:
        labels = np.array(pcd.cluster_dbscan(eps=0.005, min_points=100, print_progress=False))

    if len(labels) == 0:
        print("âš  Clustering failed or resulted in zero clusters.")
        return None, None, None, None

    # ê°€ì¥ í° í´ëŸ¬ìŠ¤í„° ID ì°¾ê¸°
    unique_labels, counts = np.unique(labels[labels != -1], return_counts=True)
    if len(unique_labels) == 0:
        print("âš  No substantial cluster found (only noise).")
        return None, None, None, None

    largest_cluster_id = unique_labels[np.argmax(counts)]
    indices = np.where(labels == largest_cluster_id)[0]
    pcd_filtered = pcd.select_by_index(indices)

    print(f"âœ… Filtered to largest cluster ({len(indices)} points).")

    # 2. PCA ê³„ì‚° (ë²•ì„  ì¶”ì •)
    points = np.asarray(pcd_filtered.points)
    center = np.mean(points, axis=0)

    # ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚°
    centered_points = points - center
    cov_matrix = np.cov(centered_points, rowvar=False)

    # ê³ ìœ ê°’ ë° ê³ ìœ  ë²¡í„° ê³„ì‚°
    eigen_values, eigen_vectors = np.linalg.eigh(cov_matrix)

    # ê³ ìœ ê°’ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (ê°€ì¥ ì‘ì€ ê³ ìœ ê°’ -> V3, ë²•ì„  ë²¡í„°)
    sort_indices = np.argsort(eigen_values)
    pca_v3_normal = eigen_vectors[:, sort_indices[0]]  # V3 (PCA Normal)
    pca_v1 = eigen_vectors[:, sort_indices[2]]         # V1 (Principal Axis)

    print(f"âœ… PCA Normal (V3) calculated: {pca_v3_normal}")

    # 3. PCA ê¸°ë°˜ RANSAC í”¼íŒ…
    distance_threshold = 0.005  # MAX_RANSAC_DISTANCE
    max_iterations = 1000

    plane_model, inliers = pcd_filtered.segment_plane(
        distance_threshold=distance_threshold,
        ransac_n=3,
        num_iterations=max_iterations
    )

    [A, B, C, D] = plane_model
    ransac_normal = np.array([A, B, C])

    # PCA ë²•ì„ ê³¼ RANSAC ë²•ì„ ì˜ ë°©í–¥ì„ ì¼ì¹˜ì‹œí‚µë‹ˆë‹¤.
    if np.dot(ransac_normal, pca_v3_normal) < 0:
        ransac_normal = -ransac_normal
        A, B, C = ransac_normal

    print(f"âœ… RANSAC Final Normal: {[A, B, C]}")

    return center, ransac_normal, pca_v1, pcd_filtered


# =========================================================================
# === Point Cloud I/O and Utility Functions ===
# =========================================================================

def save_pointcloud_from_all(point_cloud, filename):
    # ê¸°ì¡´ ì½”ë“œ ìœ ì§€
    if point_cloud.get_memory_type() == sl.MEM.GPU:
        import cupy as cp
        data_np = cp.asnumpy(cp.asarray(point_cloud.get_data(memory_type=sl.MEM.GPU, deep_copy=False)))
    else:
        data_np = point_cloud.get_data(memory_type=sl.MEM.CPU, deep_copy=False)

    all_points = data_np.reshape(-1, 4)
    valid_mask = ~np.isnan(all_points[:, 0]) & ~np.isnan(all_points[:, 1]) & ~np.isnan(all_points[:, 2])
    valid_points = all_points[valid_mask, :3]

    if len(valid_points) == 0:
        print("âš  No valid points found in the entire point cloud.")
        return

    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(valid_points)

    o3d.io.write_point_cloud(filename, pcd)
    print(f"âœ… Saved full point cloud ({len(valid_points)} points) â†’ {filename}")


# CuPy ì‚¬ìš© ì—¬ë¶€
if gl.GPU_ACCELERATION_AVAILABLE:
    import cupy as cp

# ì „ì—­ ìƒíƒœë“¤ (ìŠ¤ë ˆë“œ/Grab ë£¨í”„ ê³µìœ )
lock = Lock()
run_signal = False
exit_signal = False
image_net = None
detections = []


def save_pointcloud_from_bbox(point_cloud, bbox, filename, image_resolution, point_cloud_resolution):
    # 1. ìŠ¤ì¼€ì¼ë§ íŒ©í„° ê³„ì‚°
    scale_x = point_cloud_resolution.width / image_resolution.width
    scale_y = point_cloud_resolution.height / image_resolution.height

    # 2. ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œë¥¼ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ í•´ìƒë„ë¡œ ìŠ¤ì¼€ì¼ë§
    bbox_scaled = bbox.copy()
    bbox_scaled[:, 0] = bbox[:, 0] * scale_x
    bbox_scaled[:, 1] = bbox[:, 1] * scale_y

    # 3. ìŠ¤ì¼€ì¼ë§ëœ ë°”ìš´ë”© ë°•ìŠ¤ë¡œë¶€í„° ì˜ì—­ ì •ì˜
    x_min_scaled = int(max(0, min(bbox_scaled[:, 0])))
    x_max_scaled = int(min(point_cloud_resolution.width, max(bbox_scaled[:, 0])))
    y_min_scaled = int(max(0, min(bbox_scaled[:, 1])))
    y_max_scaled = int(min(point_cloud_resolution.height, max(bbox_scaled[:, 1])))

    points = []

    # 4. ìŠ¤ì¼€ì¼ë§ëœ ì˜ì—­ì—ì„œ í¬ì¸íŠ¸ ì¶”ì¶œ
    for y in range(y_min_scaled, y_max_scaled):
        for x in range(x_min_scaled, x_max_scaled):
            success, value = point_cloud.get_value(x, y)
            if success == sl.ERROR_CODE.SUCCESS:
                X, Y, Z, _ = value
                if not np.isnan(X) and not np.isnan(Y) and not np.isnan(Z):
                    points.append([X, Y, Z])

    if len(points) == 0:
        print("âš  No valid points found in bbox after scaling.")
        return None

    # Save to PCD
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(np.array(points))

    o3d.io.write_point_cloud(filename, pcd)
    print(f"âœ… Saved point cloud â†’ {filename}")
    return pcd


def depth_to_color(z, z_min=0.2, z_max=5.0):
    """
    Z-depth -> RGB color (blue=near, red=far)
    """
    z = np.clip((z - z_min) / (z_max - z_min), 0.0, 1.0)
    r = (z * 255).astype(np.uint8)
    g = ((1 - z) * 255).astype(np.uint8)
    b = np.full_like(r, 0)
    return (b, g, r)


def xywh2abcd(xywh):
    output = np.zeros((4, 2))

    # Center / Width / Height -> BBox corners coordinates
    x_min = max(0, xywh[0] - 0.5 * xywh[2])
    x_max = (xywh[0] + 0.5 * xywh[2])
    y_min = max(0, xywh[1] - 0.5 * xywh[3])
    y_max = (xywh[1] + 0.5 * xywh[3])

    # A ------ B
    # | Object |
    # D ------ C

    output[0][0] = x_min
    output[0][1] = y_min

    output[1][0] = x_max
    output[1][1] = y_min

    output[2][0] = x_max
    output[2][1] = y_max

    output[3][0] = x_min
    output[3][1] = y_max
    return output


def detections_to_custom_box(detections):
    output = []
    for det in detections:
        xywh = det.xywh[0]

        # Creating ingestable objects for the ZED SDK
        obj = sl.CustomBoxObjectData()
        obj.bounding_box_2d = xywh2abcd(xywh)
        obj.label = det.cls
        obj.probability = det.conf
        obj.is_grounded = False
        output.append(obj)
    return output


# =========================================================================
# === YOLO ì¶”ë¡  ìŠ¤ë ˆë“œ ===
# =========================================================================

def torch_thread(weights, img_size, conf_thres=0.2, iou_thres=0.45):
    global image_net, exit_signal, run_signal, detections

    print("Intializing Network...")

    model = YOLO(weights)

    while not exit_signal:
        if run_signal:
            lock.acquire()

            if gl.GPU_ACCELERATION_AVAILABLE:
                img_cupy = cp.asarray(image_net)[:, :, :3]  # Remove alpha channel on GPU
                img = cp.asnumpy(img_cupy)
            else:
                img = cv2.cvtColor(image_net, cv2.COLOR_RGBA2RGB)

            det = model.predict(img, save=False, imgsz=img_size, conf=conf_thres, iou=iou_thres, verbose=False)[
                0].cpu().numpy().boxes

            # ZED CustomBox format
            detections = detections_to_custom_box(det)
            lock.release()
            run_signal = False
        sleep(0.005)


def start_detection_thread(opt):
    """YOLO ì¶”ë¡  ìŠ¤ë ˆë“œë¥¼ ì‹œì‘í•˜ê³  Thread ê°ì²´ë¥¼ ë°˜í™˜"""
    capture_thread = Thread(
        target=torch_thread,
        kwargs={
            'weights': opt.weights,
            'img_size': opt.img_size,
            'conf_thres': opt.conf_thres
        }
    )
    capture_thread.start()
    return capture_thread


# =========================================================================
# === ZED / Viewer ì´ˆê¸°í™” ===
# =========================================================================

def get_memory_type(opt):
    """GPU / CPU ë©”ëª¨ë¦¬ íƒ€ì… ê²°ì •"""
    use_gpu = gl.GPU_ACCELERATION_AVAILABLE and not opt.disable_gpu_data_transfer
    mem_type = sl.MEM.GPU if use_gpu else sl.MEM.CPU

    if use_gpu:
        print("ğŸš€ Using GPU data transfer with CuPy")
    else:
        print("ğŸ’» Using CPU data transfer")
    return mem_type


def initialize_camera_and_viewers(opt, mem_type):
    """ì¹´ë©”ë¼, ë·°ì–´, ê´€ë ¨ ê°ì²´ë“¤ ì „ë¶€ ì´ˆê¸°í™”í•˜ê³  dictë¡œ ë°˜í™˜"""
    print("Initializing Camera...")

    zed = sl.Camera()

    input_type = sl.InputType()
    if opt.svo is not None:
        input_type.set_from_svo_file(opt.svo)

    # Init parameters
    init_params = sl.InitParameters(input_t=input_type, svo_real_time_mode=True)
    init_params.coordinate_units = sl.UNIT.METER
    init_params.depth_mode = sl.DEPTH_MODE.NEURAL
    init_params.coordinate_system = sl.COORDINATE_SYSTEM.RIGHT_HANDED_Y_UP
    init_params.depth_maximum_distance = 50
    init_params.depth_stabilization = 30

    runtime_params = sl.RuntimeParameters()
    status = zed.open(init_params)

    if status != sl.ERROR_CODE.SUCCESS:
        print(repr(status))
        sys.exit(1)

    image_left_tmp = sl.Mat(0, 0, sl.MAT_TYPE.U8_C4, mem_type)

    print("Initialized Camera")

    positional_tracking_parameters = sl.PositionalTrackingParameters()
    zed.enable_positional_tracking(positional_tracking_parameters)

    obj_param = sl.ObjectDetectionParameters()
    obj_param.detection_model = sl.OBJECT_DETECTION_MODEL.CUSTOM_BOX_OBJECTS
    obj_param.enable_tracking = True
    obj_param.enable_segmentation = False
    zed.enable_object_detection(obj_param)

    objects = sl.Objects()
    obj_runtime_param = sl.CustomObjectDetectionRuntimeParameters()

    # Display
    camera_infos = zed.get_camera_information()
    camera_res = camera_infos.camera_configuration.resolution

    # OpenGL viewer
    viewer = gl.GLViewer()
    point_cloud_res = sl.Resolution(min(camera_res.width, 720), min(camera_res.height, 404))
    viewer.init(camera_infos.camera_model, point_cloud_res, obj_param.enable_tracking)
    point_cloud = sl.Mat(point_cloud_res.width, point_cloud_res.height, sl.MAT_TYPE.F32_C4, mem_type)
    image_left = sl.Mat(0, 0, sl.MAT_TYPE.U8_C4, mem_type)

    # 2D display
    display_resolution = sl.Resolution(min(camera_res.width, 1280), min(camera_res.height, 720))
    image_scale = [display_resolution.width / camera_res.width, display_resolution.height / camera_res.height]
    image_left_ocv = np.full((display_resolution.height, display_resolution.width, 4),
                             [245, 239, 239, 255], np.uint8)

    # Tracks view
    camera_config = camera_infos.camera_configuration
    tracks_resolution = sl.Resolution(400, display_resolution.height)
    track_view_generator = cv_viewer.TrackingViewer(tracks_resolution, camera_config.fps,
                                                    init_params.depth_maximum_distance)
    track_view_generator.set_camera_calibration(camera_config.calibration_parameters)
    image_track_ocv = np.zeros((tracks_resolution.height, tracks_resolution.width, 4), np.uint8)

    cam_w_pose = sl.Pose()

    ctx = {
        "zed": zed,
        "runtime_params": runtime_params,
        "image_left_tmp": image_left_tmp,
        "obj_param": obj_param,
        "objects": objects,
        "obj_runtime_param": obj_runtime_param,
        "camera_infos": camera_infos,
        "camera_res": camera_res,
        "viewer": viewer,
        "point_cloud_res": point_cloud_res,
        "point_cloud": point_cloud,
        "image_left": image_left,
        "display_resolution": display_resolution,
        "image_scale": image_scale,
        "image_left_ocv": image_left_ocv,
        "tracks_resolution": tracks_resolution,
        "track_view_generator": track_view_generator,
        "image_track_ocv": image_track_ocv,
        "cam_w_pose": cam_w_pose,
    }

    return ctx


# =========================================================================
# === ë Œë”ë§/ì²˜ë¦¬ìš© í—¬í¼ ===
# =========================================================================

def colorize_bbox_depth(objects, point_cloud, image_left_ocv):
    """bbox ë‚´ë¶€ í”½ì…€ì— ê¹Šì´ ê¸°ë°˜ ì»¬ëŸ¬ ì…íˆê¸°"""
    for obj in objects.object_list:
        if obj.label == 1 or obj.label == 0:  # í´ë˜ìŠ¤ 1ê³¼ 0 ëª¨ë‘ ì‹œê°í™”
            bbox = np.array(obj.bounding_box_2d).astype(int)

            x_min = max(0, np.min(bbox[:, 0]))
            x_max = min(image_left_ocv.shape[1] - 1, np.max(bbox[:, 0]))
            y_min = max(0, np.min(bbox[:, 1]))
            y_max = min(image_left_ocv.shape[0] - 1, np.max(bbox[:, 1]))

            for y in range(y_min, y_max):
                for x in range(x_min, x_max):
                    success, value = point_cloud.get_value(x, y)
                    if success == sl.ERROR_CODE.SUCCESS:
                        X, Y, Z, _ = value
                        if not np.isnan(Z) and Z > 0:
                            b, g, r = depth_to_color(Z)
                            cv2.circle(image_left_ocv, (x, y), 1, (int(b), int(g), int(r)), -1)


def process_and_save_topmost_class1(objects, point_cloud, camera_infos, point_cloud_res, image_left_ocv):
    """s í‚¤ ì…ë ¥ ì‹œ: ê°€ì¥ ìœ„(í™”ë©´ ê¸°ì¤€) Pallet class1 í•˜ë‚˜ ì„ íƒ í›„ ì €ì¥/ì²˜ë¦¬"""
    print("ğŸ“¥ Saving point cloud and processing for the TOPMOST CLASS 1 object...")

    original_res = camera_infos.camera_configuration.resolution
    pc_res = point_cloud_res

    # ìµœìƒë‹¨ ê°ì²´ ì°¾ê¸°
    topmost_obj = None
    min_y_min = float('inf')

    for obj in objects.object_list:
        if obj.raw_label == 1 or obj.label == 1:
            bbox_2d = np.array(obj.bounding_box_2d)
            y_min = np.min(bbox_2d[:, 1])

            if y_min < min_y_min:
                min_y_min = y_min
                topmost_obj = obj

    if topmost_obj is None:
        print("âš ï¸ No object with class ID 1 detected.")
        return

    bbox_2d = np.array(topmost_obj.bounding_box_2d)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    # 1. í¬ì¸íŠ¸í´ë¼ìš°ë“œ ì €ì¥ + O3D PCD ì–»ê¸°
    filename_pcd_bbox = f"pointcloud_class1_BBOX_{timestamp}.pcd"
    pcd_object = save_pointcloud_from_bbox(point_cloud, bbox_2d, filename_pcd_bbox, original_res, pc_res)
    save_pointcloud_from_all(point_cloud, f"pointcloud_class1_{timestamp}.pcd")  # ì „ì²´ í´ë¼ìš°ë“œ ì €ì¥

    # 2. PCA / RANSAC ì²˜ë¦¬
    if pcd_object is not None:
        center, normal, pca_v1, pcd_filtered = process_class1_pointcloud(pcd_object)

        if center is not None:
            print("--------------------------------------------------")
            print("ğŸŒŸ FINAL CLASS 1 PROCESSING RESULTS ğŸŒŸ")
            print(f"Center Point: {center}")
            print(f"Normal Vector (RANSAC/PCA): {normal}")
            print(f"Principal Axis (PCA V1): {pca_v1}")
            print("--------------------------------------------------")

    # 3. Bounding Box ì˜ì—­ ì´ë¯¸ì§€ ì €ì¥
    bbox_int = bbox_2d.astype(int)
    x_min = max(0, np.min(bbox_int[:, 0]))
    x_max = min(image_left_ocv.shape[1], np.max(bbox_int[:, 0]))
    y_min = max(0, np.min(bbox_int[:, 1]))
    y_max = min(image_left_ocv.shape[0], np.max(bbox_int[:, 1]))

    cropped_image = image_left_ocv[y_min:y_max, x_min:x_max]

    if cropped_image.size > 0:
        filename_img = f"image_class1_{timestamp}.png"
        cv2.imwrite(filename_img, cropped_image)
        print(f"ğŸ–¼ï¸ Saved image crop â†’ {filename_img}")
    else:
        print("âš  Cropped image is empty.")


# =========================================================================
# === í”„ë ˆì„ ë‹¨ìœ„ ì²˜ë¦¬ í•¨ìˆ˜ ===
# =========================================================================

def process_single_frame(ctx, mem_type):
    """
    í•œ í”„ë ˆì„ ë‹¨ìœ„ë¡œ:
      - ì´ë¯¸ì§€ grab
      - YOLO ê²°ê³¼ ingest
      - point cloud / image / tracking ì—…ë°ì´íŠ¸
      - s/q í‚¤ ì²˜ë¦¬
    """
    global image_net, exit_signal, run_signal, detections

    zed = ctx["zed"]
    runtime_params = ctx["runtime_params"]
    image_left_tmp = ctx["image_left_tmp"]
    obj_param = ctx["obj_param"]
    objects = ctx["objects"]
    obj_runtime_param = ctx["obj_runtime_param"]
    camera_infos = ctx["camera_infos"]
    point_cloud_res = ctx["point_cloud_res"]
    point_cloud = ctx["point_cloud"]
    image_left = ctx["image_left"]
    display_resolution = ctx["display_resolution"]
    image_scale = ctx["image_scale"]
    image_left_ocv = ctx["image_left_ocv"]
    track_view_generator = ctx["track_view_generator"]
    image_track_ocv = ctx["image_track_ocv"]
    cam_w_pose = ctx["cam_w_pose"]

    # Grab
    if zed.grab(runtime_params) > sl.ERROR_CODE.SUCCESS:
        exit_signal = True
        return False

    # ì´ë¯¸ì§€ ê°€ì ¸ì™€ì„œ YOLO ìŠ¤ë ˆë“œì— ë˜ì§
    lock.acquire()
    zed.retrieve_image(image_left_tmp, sl.VIEW.LEFT, mem_type)
    image_net = image_left_tmp.get_data(memory_type=mem_type, deep_copy=False)
    lock.release()
    run_signal = True

    # YOLO ì¶”ë¡  ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
    while run_signal:
        sleep(0.001)

    # YOLO ê²°ê³¼ ingest
    lock.acquire()
    zed.ingest_custom_box_objects(detections)
    lock.release()
    zed.retrieve_custom_objects(objects, obj_runtime_param)

    # ëìŠ¤ / ì´ë¯¸ì§€ / pose
    zed.retrieve_measure(point_cloud, sl.MEASURE.XYZRGBA, mem_type, point_cloud_res)
    zed.retrieve_image(image_left, sl.VIEW.LEFT, mem_type, display_resolution)
    zed.get_position(cam_w_pose, sl.REFERENCE_FRAME.WORLD)

    # 3D ë Œë”ë§
    ctx["viewer"].updateData(point_cloud, objects)

    # 2D ì´ë¯¸ì§€ ë³µì‚¬
    if mem_type == sl.MEM.GPU:
        ctx["image_left_ocv"][:] = cp.asnumpy(cp.asarray(image_left.get_data(memory_type=mem_type, deep_copy=False)))
    else:
        np.copyto(image_left_ocv, image_left.get_data(memory_type=mem_type, deep_copy=False))

    # 2D bbox & ID ë Œë”ë§
    cv_viewer.render_2D(image_left_ocv, image_scale, objects, obj_param.enable_tracking)

    # bbox ì˜ì—­ ê¹Šì´ ì»¬ëŸ¬ë§
    colorize_bbox_depth(objects, point_cloud, image_left_ocv)

    # Tracking view
    track_view_generator.generate_view(objects, cam_w_pose, image_track_ocv, objects.is_tracked)

    # ìµœì¢… í™”ë©´ í•©ì¹˜ê¸°
    global_image = cv2.hconcat([image_left_ocv, image_track_ocv])

    cv2.imshow("ZED | 2D View and Birds View", global_image)
    key = cv2.waitKey(1)

    # í‚¤ ì²˜ë¦¬
    if key == ord('s') or key == ord('S'):
        process_and_save_topmost_class1(objects, point_cloud, camera_infos, point_cloud_res, image_left_ocv)

    if key == 27 or key == ord('q') or key == ord('Q'):
        exit_signal = True

    return True


# =========================================================================
# === ì „ì²´ ì‹¤í–‰ ë£¨í”„ ===
# =========================================================================

def run(opt):
    """ì™¸ë¶€ì—ì„œ í˜¸ì¶œ ê°€ëŠ¥í•œ ìƒìœ„ ì‹¤í–‰ í•¨ìˆ˜"""
    global exit_signal

    mem_type = get_memory_type(opt)

    # YOLO ìŠ¤ë ˆë“œ ì‹œì‘
    capture_thread = start_detection_thread(opt)

    # ì¹´ë©”ë¼/ë·°ì–´ ì´ˆê¸°í™”
    ctx = initialize_camera_and_viewers(opt, mem_type)

    viewer = ctx["viewer"]
    zed = ctx["zed"]

    # ë©”ì¸ ë£¨í”„
    while viewer.is_available() and not exit_signal:
        if not process_single_frame(ctx, mem_type):
            break

    # ì¢…ë£Œ ì²˜ë¦¬
    viewer.exit()
    exit_signal = True
    zed.close()
    capture_thread.join()


# =========================================================================
# === ì§„ì…ì  ===
# =========================================================================

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', type=str,
                        default=r'/home/hada/piper_ws/src/piper_sdk/pytorch_yolo/best.pt')
    parser.add_argument('--svo', type=str, default=None,
                        help='optional svo file, if not passed, use the plugged camera instead')
    parser.add_argument('--img_size', type=int, default=416, help='inference size (pixels)')
    parser.add_argument('--conf_thres', type=float, default=0.4, help='object confidence threshold')
    parser.add_argument('--disable-gpu-data-transfer', action='store_true',
                        help='Disable GPU data transfer acceleration with CuPy even if CuPy is available')
    opt = parser.parse_args()

    with torch.no_grad():
        run(opt)


if __name__ == '__main__':
    main()
